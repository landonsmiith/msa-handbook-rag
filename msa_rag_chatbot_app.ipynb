{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92fcb83",
   "metadata": {},
   "source": [
    "# MSA Handbook RAG Chatbot App\n",
    "\n",
    "This notebook creates a Retrieval-Augmented Generation (RAG) chatbot app using Streamlit and OpenAI's GPT-4o. It processes a PDF handbook, builds a FAISS index, and deploys an interactive QA interface via Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit openai faiss-cpu sentence-transformers python-dotenv numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_pipeline.py\n",
    "import faiss\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "\n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=100):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "        i += chunk_size - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def store_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\", index_path=\"faiss_index\"):\n",
    "    embedder = SentenceTransformer(model_name)\n",
    "    embeddings = np.array(embedder.encode(chunks))\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "    return index\n",
    "\n",
    "# Example usage\n",
    "text = extract_text_from_pdf(\"MSA_2025_Handbook.pdf\")\n",
    "chunks = chunk_text(text)\n",
    "index = store_embeddings(chunks)\n",
    "\n",
    "with open(\"chunks.json\", \"w\") as f:\n",
    "    json.dump(chunks, f)\n",
    "\n",
    "print(f\"Stored {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tinyllama_inference.py\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "index = faiss.read_index(\"faiss_index\")\n",
    "with open(\"chunks.json\", \"r\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    return \"\\n\\n\".join([chunks[i] for i in indices[0] if i < len(chunks)])\n",
    "\n",
    "def generate_response(query):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = f\"Use the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based only on the provided context from the MSA 2025 Handbook.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d950ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from tinyllama_inference import generate_response, retrieve_context\n",
    "\n",
    "st.set_page_config(page_title=\"MSA Handbook Assistant\", page_icon=\"ðŸ“˜\")\n",
    "st.markdown(\"<h1 style='color:#2F4F4F;'>ðŸ“˜ MSA 2025 Handbook Assistant</h1>\", unsafe_allow_html=True)\n",
    "st.write(\"Ask questions about the MSA 2025 Handbook.\")\n",
    "\n",
    "query = st.text_input(\"Enter your question:\")\n",
    "\n",
    "if query:\n",
    "    with st.spinner(\"Thinking... generating response...\"):\n",
    "        response = generate_response(query)\n",
    "        context = retrieve_context(query)\n",
    "\n",
    "    st.subheader(\"ðŸ’¬ Answer\")\n",
    "    st.write(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .streamlit/secrets.toml\n",
    "OPENAI_API_KEY=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\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d539f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "streamlit\n",
    "openai\n",
    "faiss-cpu\n",
    "sentence-transformers\n",
    "python-dotenv\n",
    "numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinyllama_inference import generate_response\n",
    "generate_response(\"What is the attendance policy?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84825d",
   "metadata": {},
   "source": [
    "## âœ… Submission Notes\n",
    "\n",
    "- All files are generated from this notebook and available in the repo.\n",
    "- To run locally:\n",
    "  ```bash\n",
    "  pip install -r requirements.txt\n",
    "  streamlit run app.py\n",
    "  ```\n",
    "- The app uses GPT-4o and a FAISS-based retrieval system.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
